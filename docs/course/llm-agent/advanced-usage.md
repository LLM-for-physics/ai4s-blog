# æ¨¡å—äºŒï¼šé«˜çº§ç”¨æ³•æ¢ç´¢

## ğŸ“– æ¦‚è¿°

åœ¨æŒæ¡äº†åŸºç¡€çš„ API è°ƒç”¨åï¼Œæœ¬æ¨¡å—å°†æ·±å…¥æ¢ç´¢ LLM çš„é«˜çº§ç”¨æ³•ã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„ Prompt æ¨¡æ¿ã€ç®¡ç†å¤æ‚çš„å¤šè½®å¯¹è¯ã€å®ç°ç»“æ„åŒ–è¾“å‡ºï¼Œä»¥åŠæŒæ¡å„ç§è¿›é˜¶å¼€å‘æŠ€å·§ã€‚

## ğŸ¨ Prompt å·¥ç¨‹ä¸æ¨¡æ¿è®¾è®¡

### Prompt è®¾è®¡åŸåˆ™

æœ‰æ•ˆçš„ Prompt è®¾è®¡æ˜¯ LLM åº”ç”¨æˆåŠŸçš„å…³é”®ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ ¸å¿ƒåŸåˆ™ï¼š

1. **æ˜ç¡®æ€§**ï¼šæ¸…æ™°åœ°è¡¨è¾¾ä½ çš„éœ€æ±‚
2. **å…·ä½“æ€§**ï¼šæä¾›å…·ä½“çš„ä¾‹å­å’Œä¸Šä¸‹æ–‡
3. **ç»“æ„åŒ–**ï¼šä½¿ç”¨ä¸€è‡´çš„æ ¼å¼å’Œç»“æ„
4. **è¿­ä»£ä¼˜åŒ–**ï¼šä¸æ–­æµ‹è¯•å’Œæ”¹è¿›

### åŸºç¡€ Prompt æ¨¡æ¿

```python
class PromptTemplate:
    """åŸºç¡€ Prompt æ¨¡æ¿ç±»"""
    
    def __init__(self, template: str):
        self.template = template
    
    def format(self, **kwargs) -> str:
        return self.template.format(**kwargs)

# ç¤ºä¾‹ï¼šæ•°å­¦é—®é¢˜æ±‚è§£æ¨¡æ¿
math_template = PromptTemplate("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦åŠ©æ‰‹ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è§£å†³æ•°å­¦é—®é¢˜ï¼š

1. ç†è§£é—®é¢˜ï¼šåˆ†æé¢˜ç›®è¦æ±‚
2. åˆ¶å®šç­–ç•¥ï¼šé€‰æ‹©åˆé€‚çš„è§£é¢˜æ–¹æ³•
3. é€æ­¥æ±‚è§£ï¼šå±•ç¤ºè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹
4. éªŒè¯ç­”æ¡ˆï¼šæ£€æŸ¥ç»“æœçš„åˆç†æ€§

é—®é¢˜ï¼š{problem}

è¯·å¼€å§‹è§£ç­”ï¼š
""")

# ä½¿ç”¨ç¤ºä¾‹
problem = "æ±‚å‡½æ•° f(x) = xÂ² + 2x - 3 çš„æœ€å°å€¼"
prompt = math_template.format(problem=problem)
print(prompt)
```

### é«˜çº§ Prompt æ¨¡æ¿ï¼ˆä½¿ç”¨ Jinja2ï¼‰

```python
from jinja2 import Template

class AdvancedPromptTemplate:
    """ä½¿ç”¨ Jinja2 çš„é«˜çº§ Prompt æ¨¡æ¿"""
    
    def __init__(self, template_str: str):
        self.template = Template(template_str)
    
    def render(self, **kwargs) -> str:
        return self.template.render(**kwargs)

# å¤æ‚çš„å¯¹è¯æ¨¡æ¿
conversation_template = AdvancedPromptTemplate("""
ä½ æ˜¯ä¸€ä¸ª{{role}}ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
{% for trait in traits %}
- {{trait}}
{% endfor %}

å¯¹è¯å†å²ï¼š
{% for message in history %}
{{message.role}}: {{message.content}}
{% endfor %}

å½“å‰ä»»åŠ¡ï¼š{{task}}

{% if constraints %}
çº¦æŸæ¡ä»¶ï¼š
{% for constraint in constraints %}
- {{constraint}}
{% endfor %}
{% endif %}

è¯·å›åº”ç”¨æˆ·çš„æœ€æ–°æ¶ˆæ¯ã€‚
""")

# ä½¿ç”¨ç¤ºä¾‹
context = {
    "role": "ç‰©ç†å­¦æ•™æˆ",
    "traits": ["ä¸¥è°¨", "è€å¿ƒ", "å–„äºç”¨ç±»æ¯”è§£é‡Šå¤æ‚æ¦‚å¿µ"],
    "history": [
        {"role": "ç”¨æˆ·", "content": "ä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿ"},
        {"role": "åŠ©æ‰‹", "content": "é‡å­çº ç¼ æ˜¯é‡å­åŠ›å­¦ä¸­çš„ä¸€ä¸ªç°è±¡..."}
    ],
    "task": "è§£é‡Šé‡å­éš§é“æ•ˆåº”",
    "constraints": ["ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€", "æä¾›å®é™…åº”ç”¨ä¾‹å­"]
}

prompt = conversation_template.render(**context)
```

### Few-Shot Learning æ¨¡æ¿

```python
class FewShotTemplate:
    """Few-shot å­¦ä¹ æ¨¡æ¿"""
    
    def __init__(self, task_description: str, examples: list, input_format: str):
        self.task_description = task_description
        self.examples = examples
        self.input_format = input_format
    
    def create_prompt(self, input_data: str) -> str:
        prompt = f"{self.task_description}\n\n"
        
        # æ·»åŠ ç¤ºä¾‹
        for i, example in enumerate(self.examples, 1):
            prompt += f"ç¤ºä¾‹ {i}:\n"
            prompt += f"è¾“å…¥: {example['input']}\n"
            prompt += f"è¾“å‡º: {example['output']}\n\n"
        
        # æ·»åŠ å½“å‰è¾“å…¥
        prompt += f"ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹è¾“å…¥:\n"
        prompt += f"è¾“å…¥: {input_data}\n"
        prompt += f"è¾“å‡º: "
        
        return prompt

# æƒ…æ„Ÿåˆ†æç¤ºä¾‹
sentiment_template = FewShotTemplate(
    task_description="è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œè¾“å‡º 'ç§¯æ'ã€'æ¶ˆæ' æˆ– 'ä¸­æ€§'ã€‚",
    examples=[
        {"input": "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ„‰å¿«ï¼", "output": "ç§¯æ"},
        {"input": "è¿™éƒ¨ç”µå½±å¤ªæ— èŠäº†ï¼Œæµªè´¹æ—¶é—´ã€‚", "output": "æ¶ˆæ"},
        {"input": "ä»Šå¤©æ˜¯æ˜ŸæœŸä¸‰ã€‚", "output": "ä¸­æ€§"}
    ],
    input_format="æ–‡æœ¬"
)

# ä½¿ç”¨
text = "è¿™ä¸ªäº§å“è´¨é‡ä¸é”™ï¼Œå€¼å¾—æ¨èã€‚"
prompt = sentiment_template.create_prompt(text)
```

## ğŸ’¬ å¤šè½®å¯¹è¯ç®¡ç†

### å¯¹è¯å†å²ç®¡ç†

```python
from typing import List, Dict, Optional
from datetime import datetime
import json

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self, max_history: int = 10, max_tokens: int = 3000):
        self.messages: List[Dict[str, str]] = []
        self.max_history = max_history
        self.max_tokens = max_tokens
        self.metadata = {
            "created_at": datetime.now().isoformat(),
            "total_exchanges": 0
        }
    
    def add_message(self, role: str, content: str, metadata: Optional[Dict] = None):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        }
        
        if metadata:
            message["metadata"] = metadata
        
        self.messages.append(message)
        
        if role == "user":
            self.metadata["total_exchanges"] += 1
        
        # è‡ªåŠ¨æ¸…ç†å†å²
        self._cleanup_history()
    
    def _cleanup_history(self):
        """æ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²"""
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in self.messages if msg["role"] == "system"]
        other_messages = [msg for msg in self.messages if msg["role"] != "system"]
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(other_messages) > self.max_history:
            other_messages = other_messages[-self.max_history:]
        
        # é™åˆ¶ token æ•°é‡
        while self._estimate_tokens(system_messages + other_messages) > self.max_tokens and other_messages:
            other_messages.pop(0)
        
        self.messages = system_messages + other_messages
    
    def _estimate_tokens(self, messages: List[Dict]) -> int:
        """ä¼°ç®—æ¶ˆæ¯çš„ token æ•°é‡"""
        total_text = " ".join([msg["content"] for msg in messages])
        chinese_chars = len([c for c in total_text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(total_text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)
    
    def get_messages(self) -> List[Dict[str, str]]:
        """è·å–ç”¨äº API è°ƒç”¨çš„æ¶ˆæ¯æ ¼å¼"""
        return [{"role": msg["role"], "content": msg["content"]} for msg in self.messages]
    
    def save_to_file(self, filename: str):
        """ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶"""
        data = {
            "messages": self.messages,
            "metadata": self.metadata
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load_from_file(self, filename: str):
        """ä»æ–‡ä»¶åŠ è½½å¯¹è¯"""
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.messages = data["messages"]
        self.metadata = data["metadata"]

# ä½¿ç”¨ç¤ºä¾‹
def chat_with_memory():
    conversation = ConversationManager()
    
    # è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯
    conversation.add_message("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ•°å­¦åŠ©æ‰‹ï¼Œæ“…é•¿è§£é‡Šå¤æ‚çš„æ•°å­¦æ¦‚å¿µã€‚")
    
    # æ¨¡æ‹Ÿå¯¹è¯
    conversation.add_message("user", "ä»€ä¹ˆæ˜¯å¯¼æ•°ï¼Ÿ")
    conversation.add_message("assistant", "å¯¼æ•°æ˜¯å‡½æ•°åœ¨æŸä¸€ç‚¹çš„ç¬æ—¶å˜åŒ–ç‡...")
    
    conversation.add_message("user", "èƒ½ç»™æˆ‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿ")
    conversation.add_message("assistant", "å½“ç„¶ï¼æ¯”å¦‚å‡½æ•° f(x) = xÂ²...")
    
    # è·å–ç”¨äº API çš„æ¶ˆæ¯
    api_messages = conversation.get_messages()
    print("API æ¶ˆæ¯æ ¼å¼:")
    for msg in api_messages:
        print(f"{msg['role']}: {msg['content'][:50]}...")
```

### ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–

```python
class ContextWindowOptimizer:
    """ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
    
    def optimize_messages(self, messages: List[Dict[str, str]], 
                         preserve_system: bool = True,
                         preserve_recent: int = 2) -> List[Dict[str, str]]:
        """ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£"""
        
        if not messages:
            return messages
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æ¶ˆæ¯
        system_msgs = [msg for msg in messages if msg["role"] == "system"]
        user_msgs = [msg for msg in messages if msg["role"] == "user"]
        assistant_msgs = [msg for msg in messages if msg["role"] == "assistant"]
        
        # é‡å»ºå¯¹è¯å¯¹
        conversation_pairs = []
        for i in range(min(len(user_msgs), len(assistant_msgs))):
            conversation_pairs.append((user_msgs[i], assistant_msgs[i]))
        
        # ä¿ç•™æœ€è¿‘çš„å¯¹è¯
        recent_pairs = conversation_pairs[-preserve_recent:] if preserve_recent > 0 else []
        
        # æ„å»ºä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
        optimized = []
        
        if preserve_system and system_msgs:
            optimized.extend(system_msgs)
        
        # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å¯¹
        for user_msg, assistant_msg in recent_pairs:
            optimized.extend([user_msg, assistant_msg])
        
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
        current_tokens = self._estimate_tokens(optimized)
        
        if current_tokens <= self.max_tokens:
            return optimized
        
        # å¦‚æœä»ç„¶è¶…å‡ºé™åˆ¶ï¼Œè¿›ä¸€æ­¥å‹ç¼©
        return self._compress_messages(optimized)
    
    def _estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """ä¼°ç®— token æ•°é‡"""
        total_text = " ".join([msg["content"] for msg in messages])
        chinese_chars = len([c for c in total_text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(total_text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)
    
    def _compress_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """å‹ç¼©æ¶ˆæ¯å†…å®¹"""
        compressed = []
        
        for msg in messages:
            if msg["role"] == "system":
                # ç³»ç»Ÿæ¶ˆæ¯ä¿æŒä¸å˜
                compressed.append(msg)
            else:
                # å‹ç¼©å…¶ä»–æ¶ˆæ¯
                content = msg["content"]
                if len(content) > 200:
                    content = content[:200] + "..."
                
                compressed.append({
                    "role": msg["role"],
                    "content": content
                })
        
        return compressed
```

## ğŸ“Š ç»“æ„åŒ–è¾“å‡º

### JSON æ ¼å¼è¾“å‡º

```python
import json
from typing import Any, Dict, Type
from pydantic import BaseModel, ValidationError

class StructuredOutputManager:
    """ç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def generate_json(self, prompt: str, schema: Dict[str, Any], 
                     max_retries: int = 3) -> Dict[str, Any]:
        """ç”Ÿæˆç¬¦åˆæŒ‡å®š schema çš„ JSON è¾“å‡º"""
        
        # æ„å»ºåŒ…å« schema çš„ prompt
        schema_prompt = f"""
{prompt}

è¯·ä»¥ JSON æ ¼å¼å›ç­”ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹ schemaï¼š
{json.dumps(schema, indent=2, ensure_ascii=False)}

åªè¿”å› JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚
"""
        
        for attempt in range(max_retries):
            try:
                response = self.llm_client.chat([
                    {"role": "user", "content": schema_prompt}
                ])
                
                # æå– JSON
                json_str = self._extract_json(response)
                result = json.loads(json_str)
                
                # éªŒè¯ schema
                if self._validate_schema(result, schema):
                    return result
                else:
                    print(f"Schema éªŒè¯å¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•")
                    
            except (json.JSONDecodeError, Exception) as e:
                print(f"JSON è§£æå¤±è´¥: {e}ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•")
        
        raise Exception("æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„ç»“æ„åŒ–è¾“å‡º")
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå– JSON"""
        # å¯»æ‰¾ JSON ä»£ç å—
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            return text[start:end].strip()
        
        # å¯»æ‰¾èŠ±æ‹¬å·
        start = text.find("{")
        end = text.rfind("}") + 1
        
        if start != -1 and end != 0:
            return text[start:end]
        
        return text.strip()
    
    def _validate_schema(self, data: Dict, schema: Dict) -> bool:
        """ç®€å•çš„ schema éªŒè¯"""
        try:
            for key, expected_type in schema.items():
                if key not in data:
                    return False
                
                if expected_type == "string" and not isinstance(data[key], str):
                    return False
                elif expected_type == "number" and not isinstance(data[key], (int, float)):
                    return False
                elif expected_type == "array" and not isinstance(data[key], list):
                    return False
                elif expected_type == "object" and not isinstance(data[key], dict):
                    return False
            
            return True
        except:
            return False

# ä½¿ç”¨ç¤ºä¾‹
def structured_output_example():
    # å®šä¹‰è¾“å‡º schema
    analysis_schema = {
        "sentiment": "string",
        "confidence": "number",
        "keywords": "array",
        "summary": "string"
    }
    
    # åˆ›å»ºç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨
    output_manager = StructuredOutputManager(llm_client)
    
    # ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
    prompt = "åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå’Œå…³é”®ä¿¡æ¯ï¼š'è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œä»·æ ¼ä¹Ÿåˆç†ï¼Œå¼ºçƒˆæ¨èï¼'"
    
    result = output_manager.generate_json(prompt, analysis_schema)
    print("ç»“æ„åŒ–è¾“å‡º:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
```

### Pydantic æ¨¡å‹éªŒè¯

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from enum import Enum

class SentimentType(str, Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"

class TextAnalysis(BaseModel):
    """æ–‡æœ¬åˆ†æç»“æœæ¨¡å‹"""
    text: str = Field(..., description="åŸå§‹æ–‡æœ¬")
    sentiment: SentimentType = Field(..., description="æƒ…æ„Ÿå€¾å‘")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ç½®ä¿¡åº¦")
    keywords: List[str] = Field(..., description="å…³é”®è¯åˆ—è¡¨")
    summary: str = Field(..., max_length=200, description="æ‘˜è¦")
    language: Optional[str] = Field(None, description="è¯­è¨€")
    
    @validator('keywords')
    def keywords_not_empty(cls, v):
        if not v:
            raise ValueError('å…³é”®è¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        return v
    
    @validator('summary')
    def summary_not_empty(cls, v):
        if not v.strip():
            raise ValueError('æ‘˜è¦ä¸èƒ½ä¸ºç©º')
        return v

class PydanticOutputManager:
    """åŸºäº Pydantic çš„ç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def generate_structured_output(self, prompt: str, model_class: Type[BaseModel], 
                                 max_retries: int = 3) -> BaseModel:
        """ç”Ÿæˆå¹¶éªŒè¯ç»“æ„åŒ–è¾“å‡º"""
        
        # è·å–æ¨¡å‹ schema
        schema = model_class.schema()
        
        # æ„å»º prompt
        structured_prompt = f"""
{prompt}

è¯·ä»¥ JSON æ ¼å¼å›ç­”ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹æ•°æ®æ¨¡å‹ï¼š

æ¨¡å‹åç§°: {model_class.__name__}
å­—æ®µè¯´æ˜:
"""
        
        for field_name, field_info in schema["properties"].items():
            field_type = field_info.get("type", "unknown")
            description = field_info.get("description", "")
            structured_prompt += f"- {field_name} ({field_type}): {description}\n"
        
        structured_prompt += "\nåªè¿”å› JSON æ ¼å¼çš„æ•°æ®ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚"
        
        for attempt in range(max_retries):
            try:
                response = self.llm_client.chat([
                    {"role": "user", "content": structured_prompt}
                ])
                
                # æå–å¹¶è§£æ JSON
                json_str = self._extract_json(response)
                data = json.loads(json_str)
                
                # ä½¿ç”¨ Pydantic éªŒè¯
                return model_class(**data)
                
            except ValidationError as e:
                print(f"æ•°æ®éªŒè¯å¤±è´¥: {e}ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•")
            except json.JSONDecodeError as e:
                print(f"JSON è§£æå¤±è´¥: {e}ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•")
            except Exception as e:
                print(f"æœªçŸ¥é”™è¯¯: {e}ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•")
        
        raise Exception("æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„ç»“æ„åŒ–è¾“å‡º")
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå– JSON"""
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            return text[start:end].strip()
        
        start = text.find("{")
        end = text.rfind("}") + 1
        
        if start != -1 and end != 0:
            return text[start:end]
        
        return text.strip()

# ä½¿ç”¨ç¤ºä¾‹
def pydantic_example():
    output_manager = PydanticOutputManager(llm_client)
    
    prompt = """
    åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š
    "ä»Šå¤©çš„å¤©æ°”éå¸¸å¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå¤–å‡ºæ¸¸ç©ã€‚è¿™ç§å¤©æ°”è®©äººå¿ƒæƒ…æ„‰æ‚¦ï¼Œå……æ»¡æ´»åŠ›ã€‚"
    
    è¯·æä¾›è¯¦ç»†çš„æ–‡æœ¬åˆ†æç»“æœã€‚
    """
    
    try:
        result = output_manager.generate_structured_output(prompt, TextAnalysis)
        print("åˆ†æç»“æœ:")
        print(f"æƒ…æ„Ÿ: {result.sentiment}")
        print(f"ç½®ä¿¡åº¦: {result.confidence}")
        print(f"å…³é”®è¯: {result.keywords}")
        print(f"æ‘˜è¦: {result.summary}")
        
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {e}")
```

## ğŸ”§ è¿›é˜¶å¼€å‘æŠ€å·§

### å¼‚æ­¥å¤„ç†

```python
import asyncio
import aiohttp
from typing import List, Dict

class AsyncLLMClient:
    """å¼‚æ­¥ LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
    
    async def chat_async(self, messages: List[Dict[str, str]], 
                        model: str = "gpt-3.5-turbo", **kwargs) -> str:
        """å¼‚æ­¥èŠå¤©"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": messages,
            **kwargs
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status}")
    
    async def batch_chat(self, message_lists: List[List[Dict[str, str]]], 
                        **kwargs) -> List[str]:
        """æ‰¹é‡å¼‚æ­¥å¤„ç†"""
        tasks = [
            self.chat_async(messages, **kwargs) 
            for messages in message_lists
        ]
        
        return await asyncio.gather(*tasks)

# ä½¿ç”¨ç¤ºä¾‹
async def async_example():
    client = AsyncLLMClient("your-api-key", "https://api.openai.com/v1")
    
    # æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜
    questions = [
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"}]
    ]
    
    results = await client.batch_chat(questions)
    
    for i, result in enumerate(results):
        print(f"é—®é¢˜ {i+1} çš„å›ç­”: {result[:100]}...")

# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
# asyncio.run(async_example())
```

### ç¼“å­˜æœºåˆ¶

```python
import hashlib
import pickle
import os
from functools import wraps
from typing import Any, Callable

class LLMCache:
    """LLM å“åº”ç¼“å­˜"""
    
    def __init__(self, cache_dir: str = "./llm_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = str(messages) + str(sorted(kwargs.items()))
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, messages: List[Dict[str, str]], **kwargs) -> Any:
        """è·å–ç¼“å­˜"""
        cache_key = self._get_cache_key(messages, **kwargs)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        
        return None
    
    def set(self, messages: List[Dict[str, str]], result: Any, **kwargs):
        """è®¾ç½®ç¼“å­˜"""
        cache_key = self._get_cache_key(messages, **kwargs)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)

def cached_llm_call(cache: LLMCache):
    """LLM è°ƒç”¨ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(messages: List[Dict[str, str]], **kwargs):
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache.get(messages, **kwargs)
            if cached_result is not None:
                print("ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return cached_result
            
            # è°ƒç”¨å®é™…å‡½æ•°
            result = func(messages, **kwargs)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            cache.set(messages, result, **kwargs)
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache = LLMCache()

@cached_llm_call(cache)
def cached_chat(messages: List[Dict[str, str]], **kwargs) -> str:
    # è¿™é‡Œè°ƒç”¨å®é™…çš„ LLM API
    return llm_client.chat(messages, **kwargs)
```

## ğŸ“ å®è·µç»ƒä¹ 

### ç»ƒä¹  1ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

```python
class IntelligentQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.conversation = ConversationManager()
        self.output_manager = StructuredOutputManager(llm_client)
        
        # è®¾ç½®ç³»ç»Ÿè§’è‰²
        self.conversation.add_message(
            "system", 
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·é—®é¢˜å¹¶æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
        )
    
    def ask(self, question: str) -> Dict[str, Any]:
        """æé—®å¹¶è·å–ç»“æ„åŒ–å›ç­”"""
        self.conversation.add_message("user", question)
        
        # å®šä¹‰å›ç­”ç»“æ„
        answer_schema = {
            "answer": "string",
            "confidence": "number",
            "sources": "array",
            "follow_up_questions": "array"
        }
        
        # æ„å»º prompt
        prompt = f"""
        åŸºäºå¯¹è¯å†å²å›ç­”ç”¨æˆ·é—®é¢˜ï¼š{question}
        
        å¯¹è¯å†å²ï¼š
        {self._format_history()}
        
        è¯·æä¾›ç»“æ„åŒ–çš„å›ç­”ã€‚
        """
        
        result = self.output_manager.generate_json(prompt, answer_schema)
        
        # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
        self.conversation.add_message("assistant", result["answer"])
        
        return result
    
    def _format_history(self) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        messages = self.conversation.get_messages()
        formatted = []
        
        for msg in messages[-6:]:  # åªæ˜¾ç¤ºæœ€è¿‘6æ¡æ¶ˆæ¯
            if msg["role"] != "system":
                formatted.append(f"{msg['role']}: {msg['content']}")
        
        return "\n".join(formatted)

# ä½¿ç”¨ç¤ºä¾‹
qa_system = IntelligentQASystem(llm_client)

# è¿ç»­æé—®
questions = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "å®ƒæœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ",
    "æœªæ¥å‘å±•è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
]

for question in questions:
    result = qa_system.ask(question)
    print(f"é—®é¢˜: {question}")
    print(f"å›ç­”: {result['answer']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']}")
    print(f"åç»­é—®é¢˜: {result['follow_up_questions']}")
    print("-" * 50)
```

## ğŸ“š æ€»ç»“

æœ¬æ¨¡å—ä»‹ç»äº† LLM çš„é«˜çº§ç”¨æ³•ï¼ŒåŒ…æ‹¬ï¼š

1. **Prompt å·¥ç¨‹**ï¼šä»åŸºç¡€æ¨¡æ¿åˆ°å¤æ‚çš„ Few-shot å­¦ä¹ 
2. **å¯¹è¯ç®¡ç†**ï¼šå†å²ç»´æŠ¤ã€ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æŒä¹…åŒ–å­˜å‚¨
3. **ç»“æ„åŒ–è¾“å‡º**ï¼šJSON æ ¼å¼æ§åˆ¶ã€Pydantic éªŒè¯
4. **è¿›é˜¶æŠ€å·§**ï¼šå¼‚æ­¥å¤„ç†ã€ç¼“å­˜æœºåˆ¶ã€é”™è¯¯å¤„ç†

è¿™äº›æŠ€èƒ½å°†ä¸ºæ„å»ºå¤æ‚çš„ LLM åº”ç”¨å¥ å®šåšå®åŸºç¡€ã€‚

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [Agent æ„å»ºå®æˆ˜](./agent-building.md)ï¼Œå°†è¿™äº›æŠ€èƒ½æ•´åˆåˆ°å®Œæ•´çš„æ™ºèƒ½ä»£ç†ä¸­ã€‚
