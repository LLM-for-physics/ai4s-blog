# æ¨¡å—ä¸€ï¼šLLM API è°ƒç”¨åŸºç¡€

## ğŸ“– æ¦‚è¿°

æœ¬æ¨¡å—å°†æ·±å…¥ä»‹ç»å¤§è¯­è¨€æ¨¡å‹ API çš„è°ƒç”¨åŸç†ï¼Œå¸®åŠ©åŒå­¦ä»¬ç†è§£å¦‚ä½•ä¸ä¸åŒçš„ LLM ä¾›åº”å•†è¿›è¡Œäº¤äº’ã€‚æˆ‘ä»¬å°†ä»æœ€åŸºç¡€çš„æ¦‚å¿µå¼€å§‹ï¼Œé€æ­¥æŒæ¡å®é™…çš„ç¼–ç¨‹æŠ€èƒ½ã€‚

## ğŸ” LLM API è°ƒç”¨åŸç†

### ä»€ä¹ˆæ˜¯ APIï¼Ÿ

APIï¼ˆApplication Programming Interfaceï¼Œåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼‰æ˜¯ä¸åŒè½¯ä»¶ç»„ä»¶ä¹‹é—´é€šä¿¡çš„æ¡¥æ¢ã€‚LLM API å…è®¸æˆ‘ä»¬é€šè¿‡ç½‘ç»œè¯·æ±‚ä¸å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚

### HTTP è¯·æ±‚åŸºç¡€

LLM API åŸºäº HTTP åè®®å·¥ä½œï¼Œä¸»è¦ä½¿ç”¨ POST æ–¹æ³•ï¼š

```python
import requests
import json

# åŸºæœ¬çš„ HTTP POST è¯·æ±‚ç»“æ„
url = "https://api.openai.com/v1/chat/completions"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
data = {
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello, world!"}]
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
```

### æµå¼å“åº” vs æ‰¹é‡å“åº”

LLM API æ”¯æŒä¸¤ç§å“åº”æ¨¡å¼ï¼š

**æ‰¹é‡å“åº”**ï¼šç­‰å¾…å®Œæ•´å›ç­”åä¸€æ¬¡æ€§è¿”å›
```python
# æ‰¹é‡å“åº”ç¤ºä¾‹
response = requests.post(url, headers=headers, json=data)
result = response.json()
print(result['choices'][0]['message']['content'])
```

**æµå¼å“åº”**ï¼šå®æ—¶è¿”å›ç”Ÿæˆçš„å†…å®¹
```python
# æµå¼å“åº”ç¤ºä¾‹
data["stream"] = True
response = requests.post(url, headers=headers, json=data, stream=True)

for line in response.iter_lines():
    if line:
        chunk = json.loads(line.decode('utf-8').split('data: ')[1])
        if chunk['choices'][0]['delta'].get('content'):
            print(chunk['choices'][0]['delta']['content'], end='')
```

## ğŸ”‘ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### Base URLï¼ˆåŸºç¡€ URLï¼‰

Base URL æ˜¯ API æœåŠ¡çš„æ ¹åœ°å€ï¼Œä¸åŒä¾›åº”å•†æœ‰ä¸åŒçš„ç«¯ç‚¹ï¼š

```python
# ä¸åŒä¾›åº”å•†çš„ Base URL
OPENAI_BASE_URL = "https://api.openai.com/v1"
ANTHROPIC_BASE_URL = "https://api.anthropic.com/v1"
AZURE_BASE_URL = "https://your-resource.openai.azure.com"

# è‡ªå®šä¹‰éƒ¨ç½²ç¤ºä¾‹
CUSTOM_BASE_URL = "https://your-custom-deployment.com/v1"
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- å®˜æ–¹ API æœåŠ¡
- ç§æœ‰åŒ–éƒ¨ç½²
- ä»£ç†æœåŠ¡
- é•œåƒæœåŠ¡

### API Keyï¼ˆAPI å¯†é’¥ï¼‰

API Key æ˜¯èº«ä»½è®¤è¯çš„å‡­è¯ï¼Œç¡®ä¿åªæœ‰æˆæƒç”¨æˆ·æ‰èƒ½è®¿é—®æœåŠ¡ã€‚

#### å®‰å…¨æœ€ä½³å®è·µ

1. **ç¯å¢ƒå˜é‡å­˜å‚¨**
```python
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è·å– API Key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
```

2. **åˆ›å»º .env æ–‡ä»¶**
```bash
# .env æ–‡ä»¶å†…å®¹
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ZHIPU_API_KEY=your-zhipu-key-here
```

3. **é…ç½® .gitignore**
```gitignore
# ç¡®ä¿ä¸æäº¤æ•æ„Ÿä¿¡æ¯
.env
*.key
config/secrets.json
```

## ğŸ¢ ä¸»æµ LLM ä¾›åº”å•†å¯¹æ¯”

### OpenAI

**ç‰¹ç‚¹**ï¼š
- æœ€æˆç†Ÿçš„ API ç”Ÿæ€
- æ¨¡å‹ç§ç±»ä¸°å¯Œï¼ˆGPT-3.5, GPT-4, GPT-4 Turboï¼‰
- æ”¯æŒå‡½æ•°è°ƒç”¨ã€å›¾åƒç†è§£ç­‰é«˜çº§åŠŸèƒ½

**API ç¤ºä¾‹**ï¼š
```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ],
    temperature=0.7,
    max_tokens=1000
)

print(response.choices[0].message.content)
```

**å®šä»·æ¨¡å¼**ï¼šæŒ‰ token è®¡è´¹ï¼Œè¾“å…¥å’Œè¾“å‡º token ä»·æ ¼ä¸åŒ

### Anthropic (Claude)

**ç‰¹ç‚¹**ï¼š
- Constitutional AI æŠ€æœ¯ï¼Œæ›´å®‰å…¨å¯é 
- é•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆClaude-2 æ”¯æŒ 100K tokensï¼‰
- ä¼˜ç§€çš„æ¨ç†å’Œåˆ†æèƒ½åŠ›

**API ç¤ºä¾‹**ï¼š
```python
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

message = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1000,
    temperature=0.7,
    messages=[
        {"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ]
)

print(message.content[0].text)
```

### Google (Gemini)

**ç‰¹ç‚¹**ï¼š
- å¤šæ¨¡æ€èƒ½åŠ›å¼ºï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰
- ä¸ Google ç”Ÿæ€ç³»ç»Ÿæ·±åº¦é›†æˆ
- å…è´¹é¢åº¦ç›¸å¯¹è¾ƒé«˜

**API ç¤ºä¾‹**ï¼š
```python
import google.generativeai as genai

genai.configure(api_key="your-api-key")
model = genai.GenerativeModel('gemini-pro')

response = model.generate_content("è§£é‡Šæ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†")
print(response.text)
```

### å›½å†…ä¾›åº”å•†

#### ç™¾åº¦æ–‡å¿ƒä¸€è¨€
```python
import requests

def call_wenxin_api(prompt):
    # è·å– access_token
    token_url = "https://aip.baidubce.com/oauth/2.0/token"
    token_params = {
        "grant_type": "client_credentials",
        "client_id": "your_api_key",
        "client_secret": "your_secret_key"
    }
    token_response = requests.post(token_url, params=token_params)
    access_token = token_response.json()["access_token"]
    
    # è°ƒç”¨æ–‡å¿ƒä¸€è¨€ API
    api_url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions?access_token={access_token}"
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    response = requests.post(api_url, json=data)
    return response.json()["result"]
```

#### æ™ºè°± ChatGLM
```python
import zhipuai

zhipuai.api_key = "your-api-key"

response = zhipuai.model_api.invoke(
    model="chatglm_turbo",
    prompt="è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
    temperature=0.7,
    top_p=0.7,
)

print(response['data']['choices'][0]['content'])
```

## ğŸ’» å®Œæ•´å®è·µç¤ºä¾‹

### ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯

```python
import os
import requests
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from dotenv import load_dotenv

load_dotenv()

class LLMClient(ABC):
    """LLM å®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        pass

class OpenAIClient(LLMClient):
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or "https://api.openai.com/v1"
        
    def chat(self, messages: List[Dict[str, str]], model: str = "gpt-3.5-turbo", **kwargs) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": messages,
            **kwargs
        }
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=data
        )
        
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")

class AnthropicClient(LLMClient):
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        
    def chat(self, messages: List[Dict[str, str]], model: str = "claude-3-sonnet-20240229", **kwargs) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        data = {
            "model": model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", 1000),
            **{k: v for k, v in kwargs.items() if k != "max_tokens"}
        }
        
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data
        )
        
        if response.status_code == 200:
            return response.json()["content"][0]["text"]
        else:
            raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    openai_client = OpenAIClient()
    anthropic_client = AnthropicClient()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ•°å­¦åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯å¾®ç§¯åˆ†ï¼Ÿ"}
    ]
    
    try:
        # OpenAI å“åº”
        openai_response = openai_client.chat(messages, temperature=0.7)
        print("OpenAI å›ç­”:")
        print(openai_response)
        print("\n" + "="*50 + "\n")
        
        # Anthropic å“åº”
        anthropic_response = anthropic_client.chat(messages, temperature=0.7)
        print("Anthropic å›ç­”:")
        print(anthropic_response)
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
```

## ğŸ› ï¸ é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µ

### å¸¸è§é”™è¯¯å¤„ç†

```python
import time
import random
from typing import Optional

def call_llm_with_retry(client: LLMClient, messages: List[Dict[str, str]], 
                       max_retries: int = 3, backoff_factor: float = 1.0) -> Optional[str]:
    """å¸¦é‡è¯•æœºåˆ¶çš„ LLM è°ƒç”¨"""
    
    for attempt in range(max_retries):
        try:
            return client.chat(messages)
            
        except requests.exceptions.RequestException as e:
            if "rate limit" in str(e).lower():
                # é€Ÿç‡é™åˆ¶ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                wait_time = backoff_factor * (2 ** attempt) + random.uniform(0, 1)
                print(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                
            elif attempt == max_retries - 1:
                print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè°ƒç”¨å¤±è´¥: {e}")
                return None
                
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {e}")
                time.sleep(1)
                
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            return None
    
    return None
```

### Token ä½¿ç”¨ä¼˜åŒ–

```python
def estimate_tokens(text: str) -> int:
    """ç²—ç•¥ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    # è‹±æ–‡å¤§çº¦ 4 ä¸ªå­—ç¬¦ = 1 tokenï¼Œä¸­æ–‡å¤§çº¦ 1.5 ä¸ªå­—ç¬¦ = 1 token
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    other_chars = len(text) - chinese_chars
    
    return int(chinese_chars / 1.5 + other_chars / 4)

def optimize_messages(messages: List[Dict[str, str]], max_tokens: int = 3000) -> List[Dict[str, str]]:
    """ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿ä¸è¶…è¿‡ token é™åˆ¶"""
    total_tokens = sum(estimate_tokens(msg["content"]) for msg in messages)
    
    if total_tokens <= max_tokens:
        return messages
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯
    optimized = []
    if messages[0]["role"] == "system":
        optimized.append(messages[0])
        messages = messages[1:]
    
    # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹æ·»åŠ ï¼Œç›´åˆ°æ¥è¿‘ token é™åˆ¶
    current_tokens = sum(estimate_tokens(msg["content"]) for msg in optimized)
    
    for msg in reversed(messages):
        msg_tokens = estimate_tokens(msg["content"])
        if current_tokens + msg_tokens <= max_tokens:
            optimized.insert(-1 if optimized and optimized[0]["role"] == "system" else 0, msg)
            current_tokens += msg_tokens
        else:
            break
    
    return optimized
```

## ğŸ“ ç»ƒä¹ ä½œä¸š

1. **åŸºç¡€ç»ƒä¹ **ï¼šä½¿ç”¨ä¸åŒçš„ LLM ä¾›åº”å•† API å®ç°ç›¸åŒçš„å¯¹è¯åŠŸèƒ½
2. **è¿›é˜¶ç»ƒä¹ **ï¼šå®ç°ä¸€ä¸ªæ”¯æŒå¤šä¾›åº”å•†çš„ç»Ÿä¸€ LLM å®¢æˆ·ç«¯
3. **å®æˆ˜ç»ƒä¹ **ï¼šæ·»åŠ é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’Œ token ä¼˜åŒ–åŠŸèƒ½

## ğŸ”— ç›¸å…³èµ„æº

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [Anthropic API æ–‡æ¡£](https://docs.anthropic.com/)
- [Google AI Studio](https://makersuite.google.com/)
- [ç™¾åº¦åƒå¸†å¤§æ¨¡å‹å¹³å°](https://cloud.baidu.com/product/wenxinworkshop)

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [é«˜çº§ç”¨æ³•æ¢ç´¢](./advanced-usage.md)ï¼ŒæŒæ¡ Prompt å·¥ç¨‹å’Œå¤šè½®å¯¹è¯ç®¡ç†æŠ€å·§ã€‚
